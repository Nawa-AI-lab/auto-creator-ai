"""å…è´¹å›¾ç‰‡ç”Ÿæˆå™¨ - ä½¿ç”¨Hugging Face Diffusers"""
import os
import asyncio
from typing import List, Optional
from PIL import Image
import io
import base64

# å°è¯•å¯¼å…¥ï¼Œå¦‚æœä¸å¯ç”¨åˆ™è·³è¿‡
try:
    from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler
    import torch
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    print("âš ï¸ Diffusersæœªå®‰è£…ï¼Œå°†ä½¿ç”¨APIæ¨¡å¼")


class FreeImageGenerator:
    """å…è´¹å›¾ç‰‡ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.cache_dir = "generated_images"
        os.makedirs(self.cache_dir, exist_ok=True)
        self.pipe = None
        self._init_pipeline()
    
    def _init_pipeline(self):
        """åˆå§‹åŒ–æœ¬åœ°Stable Diffusion XL"""
        if not DIFFUSERS_AVAILABLE:
            return
        
        try:
            print("ğŸ“¦ æ­£åœ¨åŠ è½½Stable Diffusion XLæ¨¡å‹...")
            scheduler = EulerAncestralDiscreteScheduler(
                tau_min=0.05,
                tau_max=0.5,
                beta_min=0.00085,
                beta_max=0.012
            )
            
            self.pipe = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-base-1.0",
                torch_dtype=torch.float16,
                variant="fp16"
            )
            self.pipe.scheduler = scheduler
            self.pipe.to("cuda" if torch.cuda.is_available() else "cpu")
            print("âœ… Stable Diffusion XLåŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½æœ¬åœ°æ¨¡å‹: {e}")
            self.pipe = None
    
    async def generate_image(
        self,
        prompt: str,
        size: tuple = (1024, 1024),
        save_to_disk: bool = True
    ) -> str:
        """ç”Ÿæˆå•å¼ å›¾ç‰‡"""
        
        # å¢å¼ºæç¤ºè¯
        enhanced_prompt = self._enhance_prompt(prompt)
        
        if self.pipe and not DIFFUSERS_AVAILABLE:
            # æœ¬åœ°ç”Ÿæˆ
            return await self._generate_local(enhanced_prompt, size, save_to_disk)
        else:
            # ä½¿ç”¨Hugging Face API
            return await self._generate_via_api(enhanced_prompt, size, save_to_disk)
    
    def _enhance_prompt(self, prompt: str) -> str:
        """å¢å¼ºæç¤ºè¯ä»¥è·å¾—æ›´å¥½çš„å›¾ç‰‡"""
        
        enhancements = [
            "masterpiece, best quality",
            "highly detailed",
            "professional photography",
            "cinematic lighting",
            "8k resolution"
        ]
        
        return f"{prompt}, {', '.join(enhancements)}"
    
    async def _generate_local(
        self,
        prompt: str,
        size: tuple,
        save_to_disk: bool
    ) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        
        if not self.pipe:
            raise ValueError("æœ¬åœ°æ¨¡å‹æœªåŠ è½½")
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œä»¥é¿å…é˜»å¡
        loop = asyncio.get_event_loop()
        
        def run_inference():
            result = self.pipe(
                prompt,
                height=size[1],
                width=size[0],
                guidance_scale=7.5,
                num_inference_steps=30
            )
            return result.images[0]
        
        image = await loop.run_in_executor(None, run_inference)
        
        if save_to_disk:
            filename = f"img_{hash(prompt)}.png"
            filepath = os.path.join(self.cache_dir, filename)
            image.save(filepath, "PNG")
            return filepath
        
        # è¿”å›Base64
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        return f"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()}"
    
    async def _generate_via_api(
        self,
        prompt: str,
        size: tuple,
        save_to_disk: bool
    ) -> str:
        """ä½¿ç”¨Hugging Face Inference API"""
        
        import aiohttp
        
        async with aiohttp.ClientSession() as session:
            # ä½¿ç”¨å…è´¹çš„FLUXæ¨¡å‹æˆ–å…¶ä»–å…è´¹æ¨¡å‹
            # https://huggingface.co/spaces/black-forest-labs/FLUX.1-schnell
            
            headers = {"Authorization": f"Bearer {settings.HF_TOKEN}"}
            
            payload = {
                "inputs": prompt,
                "parameters": {
                    "width": size[0],
                    "height": size[1],
                    "guidance_scale": 7.5
                }
            }
            
            try:
                async with session.post(
                    "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-xl-base-1.0",
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        image_bytes = await response.read()
                        image = Image.open(io.BytesIO(image_bytes))
                        
                        if save_to_disk:
                            filename = f"img_{hash(prompt)}.png"
                            filepath = os.path.join(self.cache_dir, filename)
                            image.save(filepath, "PNG")
                            return filepath
                        
                        return f"data:image/png;base64,{base64.b64encode(image_bytes).decode()}"
                    else:
                        # å¦‚æœHF APIå¤±è´¥ï¼Œä½¿ç”¨å ä½å›¾
                        return self._create_placeholder(prompt, save_to_disk)
                        
            except Exception as e:
                print(f"âŒ HF APIé”™è¯¯: {e}")
                return self._create_placeholder(prompt, save_to_disk)
    
    def _create_placeholder(self, prompt: str, save_to_disk: bool) -> str:
        """åˆ›å»ºå ä½å›¾"""
        
        # åˆ›å»ºç®€å•çš„å ä½å›¾ç‰‡
        img = Image.new('RGB', (1024, 1024), color=(73, 109, 137))
        
        if save_to_disk:
            filename = f"placeholder_{hash(prompt)}.png"
            filepath = os.path.join(self.cache_dir, filename)
            img.save(filepath, "PNG")
            return filepath
        
        buffered = io.BytesIO()
        img.save(buffered, format="PNG")
        return f"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()}"
    
    async def generate_batch(
        self,
        prompts: List[str],
        parallel: bool = True
    ) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆå›¾ç‰‡"""
        
        if parallel:
            tasks = [self.generate_image(p) for p in prompts]
            results = await asyncio.gather(*tasks)
        else:
            results = []
            for prompt in prompts:
                image = await self.generate_image(prompt)
                results.append(image)
        
        return results


# åˆ›å»ºå…¨å±€å®ä¾‹
image_generator = FreeImageGenerator()
